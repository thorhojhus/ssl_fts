{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ComplexReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexReLU, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        real = self.relu(x.real)\n",
    "        imag = self.relu(x.imag)\n",
    "        return torch.complex(real, imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.fft import rfft\n",
    "\n",
    "# An implementation of the FITS model as described in https://arxiv.org/abs/2307.03756 (FITS: Frequency Interpolation Time Series Forecasting)\n",
    "# with better annotation and more clear model structure - the original code for the model can be found here: https://github.com/VEWOXIC/FITS\n",
    "\n",
    "\n",
    "class FITS(nn.Module):\n",
    "    \"\"\"Reimplementation of the FITS model.\n",
    "\n",
    "    This model contains an annotated and more cleany written model, which is true to the model described in the paper* and the original code.\n",
    "    *(RIN was not performed in the code, but instance wise normalization was performed instead).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        args: Namespace,\n",
    "    ):\n",
    "        super(FITS, self).__init__()\n",
    "\n",
    "        self.cutoff_frequency = args.dominance_freq\n",
    "        self.seq_len = args.seq_len\n",
    "        self.pred_len = args.pred_len\n",
    "        self.upsample_rate = (args.seq_len + args.pred_len) / args.seq_len\n",
    "        self.channels = args.channels\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        layers = []\n",
    "        in_features = args.dominance_freq\n",
    "        for i in range(self.num_layers):\n",
    "            out_features = (\n",
    "                int(args.dominance_freq * self.upsample_rate)\n",
    "                if self.num_layers == 1 or i == self.num_layers - 1\n",
    "                else args.num_hidden\n",
    "            )\n",
    "            layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features=in_features,\n",
    "                    out_features=out_features,\n",
    "                    dtype=torch.cfloat,\n",
    "                    bias=True,\n",
    "                )\n",
    "            )\n",
    "            if i != self.num_layers - 1:\n",
    "                layers.append(ComplexReLU())\n",
    "            in_features = out_features\n",
    "\n",
    "        self.frequency_upsampler = (\n",
    "            nn.Sequential(*layers)\n",
    "            if not args.individual\n",
    "            else nn.ModuleList([nn.Sequential(*layers) for _ in range(args.channels)])\n",
    "        )\n",
    "\n",
    "        self.individual = args.individual\n",
    "        self.debug = args.debug\n",
    "        if args.debug:\n",
    "            self.debug_tensors = {}\n",
    "\n",
    "    def channel_wise_frequency_upsampler(\n",
    "        self, ts_frequency_data_filtered: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Performs the complex valued layer frequency upsampling on a per-channel basis.\"\"\"\n",
    "        complex_valued_data = torch.zeros(\n",
    "            [\n",
    "                ts_frequency_data_filtered.size(0),\n",
    "                int(self.cutoff_frequency * self.upsample_rate),\n",
    "                ts_frequency_data_filtered.size(2),\n",
    "            ],\n",
    "            dtype=ts_frequency_data_filtered.dtype,\n",
    "        ).to(ts_frequency_data_filtered.device)\n",
    "        for i in range(self.channels):\n",
    "            complex_valued_data[:, :, i] = self.frequency_upsampler[i](\n",
    "                ts_frequency_data_filtered[:, :, i]\n",
    "            )\n",
    "        return complex_valued_data\n",
    "\n",
    "    def forward(self, ts_data: torch.Tensor) -> torch.Tensor:\n",
    "        # 1) Normalization of the input tensor:\n",
    "        ts_mean, ts_var = (\n",
    "            torch.mean(ts_data, dim=1, keepdim=True),\n",
    "            torch.var(ts_data, dim=1, keepdim=True) + 1e-5,\n",
    "        )\n",
    "        normalized_ts_data = (ts_data - ts_mean) / torch.sqrt(ts_var)\n",
    "\n",
    "        # 2) perform real fast fourier transform on the input tensor\n",
    "        ts_frequency_data = rfft(input=normalized_ts_data, dim=1)\n",
    "\n",
    "        # 3) perform a low pass filter to remove high frequency noise, which contributes little to the overall signal\n",
    "        ts_frequency_data_filtered = ts_frequency_data[:, 0 : self.cutoff_frequency, :]\n",
    "\n",
    "        # 4) Run the tensor through a complex valued linear layer\n",
    "        if self.individual:\n",
    "            complex_valued_data = self.channel_wise_frequency_upsampler(\n",
    "                ts_frequency_data_filtered\n",
    "            )\n",
    "        else:\n",
    "            complex_valued_data = self.frequency_upsampler(\n",
    "                ts_frequency_data_filtered.permute(0, 2, 1)\n",
    "            ).permute(0, 2, 1)\n",
    "\n",
    "        # 5) obtain new frequencies from the output of the complex valued linear layer\n",
    "        norm_spec_xy = torch.zeros(\n",
    "            [\n",
    "                complex_valued_data.size(0),\n",
    "                int((self.seq_len + self.pred_len) / 2 + 1),\n",
    "                complex_valued_data.size(2),\n",
    "            ],\n",
    "            dtype=complex_valued_data.dtype,\n",
    "        ).to(complex_valued_data.device)\n",
    "\n",
    "        # 6) 0 pad the output tensor\n",
    "        norm_spec_xy[:, 0 : complex_valued_data.size(1), :] = complex_valued_data\n",
    "\n",
    "        # 7) perform inverse real fast fourier transform on the output tensor\n",
    "        norm_xy = torch.fft.irfft(norm_spec_xy, dim=1)\n",
    "        norm_xy = norm_xy * self.upsample_rate\n",
    "\n",
    "        # 8) Reverse Normalization\n",
    "        xy = norm_xy * torch.sqrt(ts_var) + ts_mean\n",
    "\n",
    "        if self.debug:\n",
    "            self.debug_tensors = {\n",
    "                \"normalized_ts_data\": normalized_ts_data,\n",
    "                \"ts_frequency_data\": ts_frequency_data,\n",
    "                \"ts_frequency_data_filtered\": ts_frequency_data_filtered,\n",
    "                \"complex_valued_data\": complex_valued_data,\n",
    "                \"norm_spec_xy\": norm_spec_xy,\n",
    "                \"norm_xy\": norm_xy,\n",
    "                \"xy\": xy,\n",
    "            }\n",
    "\n",
    "        return xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthgen import SyntheticDatasetGenerator\n",
    "import numpy as np\n",
    "\n",
    "seq_len = 720\n",
    "pred_len = 720\n",
    "batch_size = 32\n",
    "n_features = 1\n",
    "n_samples = 10000\n",
    "\n",
    "ds = SyntheticDatasetGenerator(length=n_samples, num_channels=1)\n",
    "# ds.add_linear_trend(slope=-50 / n_samples, intercept=0)\n",
    "ds.add_sin_wave(amplitude=0.3, frequency=200)\n",
    "ds.add_sin_wave(amplitude=0.3, frequency=500)\n",
    "ds.add_noise(mean=0, std=0.3)\n",
    "ds.add_exponential_growth(1 + (3 / n_samples))\n",
    "ds.add_random_signal_with_precursor(\n",
    "    precursor_amplitude=2,\n",
    "    signal_amplitude=4,\n",
    "    max_precursor_length=200,\n",
    "    min_delay=150,\n",
    "    max_delay=200,\n",
    "    num_signals=int(n_samples / 500),\n",
    ")\n",
    "\n",
    "y = ds.data\n",
    "\n",
    "# normalize\n",
    "y_mean = np.mean(y, axis=0)\n",
    "y_std = np.std(y, axis=0)\n",
    "y = (y - y_mean) / y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, model, model_optim, finetune=False):\n",
    "    for epoch in range(n_epochs):\n",
    "        order = np.random.permutation(n_samples // 2)\n",
    "        train_loss = []\n",
    "        for batch in range(25):\n",
    "            batch_start = order[batch * batch_size : (batch + 1) * batch_size]\n",
    "            batch = []\n",
    "            for i in batch_start:\n",
    "                batch.append(y[i : i + seq_len + pred_len])\n",
    "            batch_xy = np.array(batch).reshape(\n",
    "                batch_size, seq_len + pred_len, n_features\n",
    "            )\n",
    "\n",
    "            batch_xy = torch.from_numpy(batch_xy).float()\n",
    "\n",
    "            batch_x = batch_xy[:, :seq_len, :]\n",
    "            batch_y = batch_xy[:, seq_len:, :]\n",
    "\n",
    "            model_optim.zero_grad()\n",
    "\n",
    "            output = model(batch_x)\n",
    "            if finetune:\n",
    "                loss = criterion(output[:, seq_len:], batch_y)\n",
    "            else:\n",
    "                loss = criterion(output, batch_xy)\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            model_optim.step()\n",
    "\n",
    "        print(f\"\\repoch: {epoch} loss: {np.mean(train_loss):1f}\", flush=True, end=\"\")\n",
    "    # print(np.mean(train_loss))\n",
    "    return model, output.cpu().detach().numpy(), batch_xy.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200\n",
      "FITS(\n",
      "  (frequency_upsampler): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=200, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 999 loss: 0.075483"
     ]
    }
   ],
   "source": [
    "conf = Namespace(\n",
    "    dominance_freq=100,\n",
    "    seq_len=seq_len,\n",
    "    pred_len=pred_len,\n",
    "    channels=1,\n",
    "    individual=False,\n",
    "    debug=False,\n",
    "    num_layers=1,\n",
    "    num_hidden=64,\n",
    ")\n",
    "\n",
    "model = FITS(conf)\n",
    "\n",
    "model_optim = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "print(model)\n",
    "\n",
    "model, out, batch = train(1000, model, model_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31944\n",
      "FITS(\n",
      "  (frequency_upsampler): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=64, bias=True)\n",
      "    (1): ComplexReLU(\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ComplexReLU(\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ComplexReLU(\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (6): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (7): ComplexReLU(\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (8): Linear(in_features=64, out_features=200, bias=True)\n",
      "  )\n",
      ")\n",
      "epoch: 999 loss: 0.011117"
     ]
    }
   ],
   "source": [
    "conf = Namespace(\n",
    "    dominance_freq=100,\n",
    "    seq_len=seq_len,\n",
    "    pred_len=pred_len,\n",
    "    channels=1,\n",
    "    individual=False,\n",
    "    debug=False,\n",
    "    num_layers=5,\n",
    "    num_hidden=64,\n",
    ")\n",
    "\n",
    "model = FITS(conf)\n",
    "\n",
    "model_optim = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "print(model)\n",
    "\n",
    "model, out, batch = train(1000, model, model_optim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
